name: News Agent (politics)

on:
  schedule:
    # Runs every hour; ET window enforced in the job (5amâ€“11pm ET)
    - cron: "0 * * * *"
  workflow_dispatch:

jobs:
  run:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Check Eastern Time window (5amâ€“11pm ET)
        run: |
          HOUR=$(TZ=America/New_York date +%H)
          if [ "$HOUR" -lt 5 ] || [ "$HOUR" -ge 23 ]; then
            echo "Outside 5amâ€“11pm ET. Exiting."
            exit 0
          fi

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Debug - List files
        run: |
          echo "Current directory:"
          pwd
          echo "Files in current directory:"
          ls -la
          echo "Checking for news_picker.py:"
          test -f news_picker.py && echo "news_picker.py EXISTS" || echo "news_picker.py MISSING"

      - name: Restore Google service account JSON
        run: |
          mkdir -p credentials
          echo "${{ secrets.GOOGLE_SERVICE_ACCOUNT_JSON_B64 }}" | base64 --decode > credentials/service_account.json
          python -m json.tool credentials/service_account.json > /dev/null || (echo "Invalid JSON" && exit 1)

      - name: Run generator (auto politics)
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          NEWSDATA_API_KEY: ${{ secrets.NEWSDATA_API_KEY }}
          GOOGLE_SHEETS_KEY: ${{ secrets.GOOGLE_SHEETS_KEY }}
          GOOGLE_SERVICE_ACCOUNT_JSON_PATH: credentials/service_account.json
          PEXELS_API_KEY: ${{ secrets.PEXELS_API_KEY }}
          AI_HORDE_API_KEY: ${{ secrets.AI_HORDE_API_KEY }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          # Dedupe knobs (Sheets-based, persistent across ephemeral CI runs)
          RUNS_DEDUPE_LOOKBACK: "200"
          USEDSTORIES_DEDUPE_LOOKBACK: "1000"
        run: |
                  # CRITICAL: Wipe generated/ to prevent stale file reuse
                  rm -rf generated
                  mkdir -p generated
                  python generate_segments.py --auto --country us --topic politics --duration 3.5 --model "gemini-2.5-flash" --max-words 15 --min-words 10

      - name: Install FFmpeg and fonts
        if: success()
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg fonts-dejavu-core
          ffmpeg -version

      - name: Verify segment generation succeeded (freshness guard)
        if: success()
        run: |
          echo "Checking for required generated files..."
          test -f generated/article.json || (echo "âŒ article.json missing - segment generation failed" && exit 1)
          test -s generated/article.json || (echo "âŒ article.json is empty" && exit 1)
          test -f generated/run_id.txt || (echo "âŒ run_id.txt missing" && exit 1)
          echo "âœ… All required files present"

      - name: Notify on rate limit failure
        if: failure()
        run: |
          echo "::error title=Content Generation Failed::Segment generation failed due to rate limits or errors. No video will be created. Check Gemini/Groq quotas."
          echo "âŒ WORKFLOW STOPPED: Unable to generate content due to LLM rate limits"
          echo "ðŸ“§ Action Required: Check your Gemini API quota at https://aistudio.google.com/app/apikey"
          echo "ðŸ“§ Check your Groq API quota at https://console.groq.com/keys"

      - name: Generate video
        if: success()
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          PEXELS_API_KEY: ${{ secrets.PEXELS_API_KEY }}
          NEWSDATA_API_KEY: ${{ secrets.NEWSDATA_API_KEY }}
          GOOGLE_SHEETS_KEY: ${{ secrets.GOOGLE_SHEETS_KEY }}
          GOOGLE_SERVICE_ACCOUNT_JSON_PATH: credentials/service_account.json
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
        run: |
          mkdir -p credentials
          echo "${{ secrets.GOOGLE_SERVICE_ACCOUNT_JSON_B64 }}" | base64 --decode > credentials/service_account.json
          python fetch_segments_for_video.py --output-dir generated
          python video/make_video.py

      - name: Generate caption with AI
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}  # harmless; mostly unused in CI
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}      # REQUIRED for Groq fallback
        run: |
          python generate_caption.py generated
          TIMESTAMP=$(date +%s)
          export TIMESTAMP
          python generate_video_metadata.py
          echo "$TIMESTAMP" > generated/.timestamp


      # Authenticate first so subsequent CLI setup and commands pick up creds
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Verify active gcloud account
        run: |
          echo "Currently authenticated account:"
          gcloud config get-value account
          echo "List of active accounts:"
          gcloud auth list --filter=status:ACTIVE

      - name: Show deployer SA email (debug)
        run: |
          python - <<'PY'
          import json, os
          p=os.environ.get('GOOGLE_APPLICATION_CREDENTIALS') or os.environ.get('GOOGLE_GHA_CREDS_PATH')
          print("DEPLOYER_SA_EMAIL=" + json.load(open(p))['client_email'])
          PY

      - name: Setup Google Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}

      - name: Verify GCP auth and bucket access (preflight)
        env:
          GCS_BUCKET: ${{ secrets.GCS_BUCKET }}
        run: |
          echo "Active gcloud account:"
          gcloud auth list --filter=status:ACTIVE
          echo "Current project: $(gcloud config get-value project 2>/dev/null)"
          echo "Listing bucket metadata (if accessible): gs://$GCS_BUCKET"
          gcloud storage ls -b gs://$GCS_BUCKET || echo "Note: Could not list bucket metadata; continuing to upload step"

      - name: Show IAM fix commands (if upload gets 403)
        if: always()
        env:
          GCS_BUCKET: ${{ secrets.GCS_BUCKET }}
        run: |
          # Derive the service account email from the credentials file created by the auth action
          SA_EMAIL=$(python -c "import json, os; p=os.environ.get('GOOGLE_APPLICATION_CREDENTIALS') or os.environ.get('GOOGLE_GHA_CREDS_PATH'); print(json.load(open(p))['client_email'])")
          echo "If the next step fails with 403, grant these roles to the service account on your bucket:"
          echo "Service Account: ${SA_EMAIL}"
          echo "Bucket: gs://${GCS_BUCKET}"
          echo "---- Run in Cloud Shell (copy/paste) ----"
          echo "gcloud storage buckets add-iam-policy-binding gs://${GCS_BUCKET} --member=serviceAccount:${SA_EMAIL} --role=roles/storage.objectCreator"
          echo "gcloud storage buckets add-iam-policy-binding gs://${GCS_BUCKET} --member=serviceAccount:${SA_EMAIL} --role=roles/storage.objectViewer"
          echo "# Alternatively, a single broader role:"
          echo "# gcloud storage buckets add-iam-policy-binding gs://${GCS_BUCKET} --member=serviceAccount:${SA_EMAIL} --role=roles/storage.objectAdmin"

      - name: Upload video to Google Cloud Storage
        run: |
          # Copy final video and thumbnail to generated folder with timestamp
          TIMESTAMP=$(cat generated/.timestamp)
          cp output/final.mp4 generated/news_video_${TIMESTAMP}.mp4
          
          # NEW: Prepare the thumbnail (using the first slide)
          if [ -f generated/0001.jpg ]; then
            cp generated/0001.jpg generated/news_video_${TIMESTAMP}.jpg
          fi
          
          # Upload to GCS (triggers Cloud Function automatically)
          # 1. Upload Video
          gcloud storage cp generated/news_video_${TIMESTAMP}.mp4 gs://${{ secrets.GCS_BUCKET }}/incoming/
          
          # 2. Upload Thumbnail (NEW)
          if [ -f generated/news_video_${TIMESTAMP}.jpg ]; then
            gcloud storage cp generated/news_video_${TIMESTAMP}.jpg gs://${{ secrets.GCS_BUCKET }}/incoming/
          fi
          
          # 3. Upload Metadata JSON (This triggers the function last)
          if [ -f generated/news_video_${TIMESTAMP}.json ]; then
            gcloud storage cp generated/news_video_${TIMESTAMP}.json gs://${{ secrets.GCS_BUCKET }}/incoming/
          fi
          
          echo "âœ… Video and assets uploaded to GCS"

      - name: Upload video artifact (backup)
        uses: actions/upload-artifact@v4
        with:
          name: news-video-${{ github.run_number }}
          path: |
            generated/news_video_*.mp4
            generated/caption.json
          retention-days: 30

      - name: Build Cloud Function (gcs-to-social)
        run: |
          mkdir -p build/gcs-to-social
          cp uploader/main.py build/gcs-to-social/
          cp uploader/requirements.txt build/gcs-to-social/
          cp uploader/caption_utils.py build/gcs-to-social/
          [ -d uploader/utils ] && cp -r uploader/utils build/gcs-to-social/ || true

      - name: Prepare clean function build
        run: |
          rm -rf build/gcs-to-social
          mkdir -p build/gcs-to-social
          cp uploader/main.py build/gcs-to-social/
          cp uploader/requirements.txt build/gcs-to-social/
          cp uploader/caption_utils.py build/gcs-to-social/
          [ -d uploader/utils ] && cp -r uploader/utils build/gcs-to-social/ || true

      - name: Verify no legacy tuple-unpack remains
        run: |
          set -e
          echo "Scanning for 'title, caption = _build_caption'â€¦"
          if grep -R --line-number -E "title\s*,\s*caption\s*=\s*_build_caption" build/gcs-to-social; then
            echo "ERROR: Found legacy tuple-unpack usage. Fix it before deploy."
            exit 1
          fi
          echo "OK: No legacy tuple-unpack found."


      - name: Deploy Cloud Function (gcs-to-social)
        run: |
          gcloud functions deploy gcs-to-social \
            --region=us-central1 \
            --runtime=python311 \
            --entry-point=gcs_to_social \
            --trigger-bucket=${{ secrets.GCS_BUCKET }} \
            --memory=1Gi \
            --timeout=540s \
            --retry \
            --set-secrets=FACEBOOK_PAGE_TOKEN=FACEBOOK_PAGE_TOKEN:latest,FB_PAGE_ID=FB_PAGE_ID:latest,GEMINI_API_KEY=GEMINI_API_KEY:latest,YOUTUBE_CREDENTIALS_JSON=YOUTUBE_CREDENTIALS_JSON:latest,MAKE_WEBHOOK_URL=MAKE_WEBHOOK_URL:latest,GROQ_API_KEY=GROQ_API_KEY:latest \
            --set-env-vars=GCP_PROJECT=${{ secrets.GCP_PROJECT_ID }},GOOGLE_CLOUD_PROJECT=${{ secrets.GCP_PROJECT_ID }} \
            --source=build/gcs-to-social
